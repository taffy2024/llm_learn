#æ³¨æ„è¦ä¼ å…¥çš„å‚æ•°ğŸ˜“
import torch
from torch import nn
import torch.nn.functional as F
import math
X=torch.randn(128,64,512) #  éšæœºç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º(batch=128,time=64,dimension=512)çš„ä¸‰ç»´å¼ é‡
print(X.shape)

special_tokens = ["<UNK>", "<PAD>", "<SOS>", "<EOS>"]
vocab = {"<UNK>":0,"<PAD>":1,"<SOS>":2,"<EOS>":3}  #æœªçŸ¥è¯ï¼Œå¡«å……æ ‡è®°ï¼Œåºåˆ—å¼€å§‹ï¼Œåºåˆ—ç»“æŸ
d_model=512 #æ¯ä¸ªè¯å…ƒè¢«è¡¨ç¤ºæˆä¸€ä¸ª 512 ç»´çš„å‘é‡
n_head=8 #æ³¨æ„åŠ›å¤´æ•°ç›®
# è¯åµŒå…¥å±‚ï¼šå°†åºåˆ—ä¸­çš„æ¯ä¸ªç¦»æ•£è¯å…ƒï¼Œåˆ†åˆ«è½¬åŒ–ä¸ºä¸€ä¸ªd_modelç»´çš„å‘é‡ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªè¯å‘é‡åºåˆ—
class TokenEmbedding(nn.Embedding):
    def __init__(self,vocab_size,d_model): #vocab_size:è¯æ±‡è¡¨å¤§å°(ä¸åŒçš„tokenæ€»æ•°)
        super(TokenEmbedding,self).__init__(vocab_size,d_model,padding_idx=1) 
# è¾“å…¥å½¢çŠ¶ä¸º(batch_size,seq_len)çš„è¯å…ƒç´¢å¼•åºåˆ—
# è¾“å‡ºå½¢çŠ¶ä¸º(batch_size,seq_len,d_model)çš„è¯å‘é‡åºåˆ—
#padding_idx=1ç¡®ä¿ç´¢å¼•ä¸º1çš„å¡«å……ç¬¦å·å§‹ç»ˆæ˜ å°„ä¸ºå…¨ 0 å‘é‡ï¼Œé¿å…å…¶å¹²æ‰°æ¨¡å‹å¯¹æœ‰æ•ˆè¯å…ƒçš„å­¦ä¹ ã€‚
class PositionalEmbedding(nn.Module):
    def __init__(self,d_model,maxlen,device):
        super(PositionalEmbedding,self).__init__()
        self.encoding=torch.zeros(maxlen,d_model,device=device) #åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„äºŒç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º(maxlen,d_model)
        self.encoding.requires_grad_(False)  #ä¸ä½¿ç”¨æ¢¯åº¦
        
        # ç”Ÿæˆä½ç½®ç´¢å¼•(0åˆ°maxlen-1)
        pos=torch.arange(0,maxlen,device=device) #arangeï¼šç”Ÿæˆç­‰å·®æ•°åˆ—çš„å¼ é‡
        pos=pos.float().unsqueeze(1) #åœ¨ç¬¬1ç»´åº¦æ’å…¥æ–°ç»´åº¦
        #ç”Ÿæˆç´¢å¼•ç»´åº¦(æ­¥é•¿ä¸º2ï¼Œå¯¹åº”å¶æ•°ç»´åº¦)
        _2i=torch.arange(0,d_model,2,device=device)
        # å¶æ•°ç»´åº¦
        self.encoding[:,0::2]=torch.sin(pos/(10000**(_2i/d_model)))
        # å¥‡æ•°ç»´åº¦
        self.encoding[:,1::2]=torch.cos(pos/(10000**(_2i/d_model)))

    def forward(self,x):
        seq_len=x.shape[1] # x æ˜¯è¯å‘é‡åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)ï¼Œå–å®é™…åºåˆ—é•¿åº¦
        return self.encoding[:seq_len,:] # è¿”å›å‰ seq_len ä¸ªä½ç½®çš„åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (seq_len, d_model)
class TransformerEmbedding(nn.Module):
    def __init__(self,vocab_size,d_model,maxlen,drop_prob,device):
        super(TransformerEmbedding,self).__init__()
        self.token=TokenEmbedding(vocab_size,d_model)
        self.position=PositionalEmbedding(d_model,maxlen,device)
        self.drop_out=nn.Dropout(p=drop_prob) #è®­ç»ƒæ—¶éšæœºå¤±æ´»éƒ¨åˆ†å…ƒç´ ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

    def forward(self,x):
        token=self.token(x)
        position=self.position(x)
        return self.drop_out(token+position)
class LayerNorm(nn.Module):
    def __init__(self,d_model,eps=1e-10):
        super(LayerNorm,self).__init__()
        self.gamma=nn.Parameter(torch.ones(d_model))
        self.beta=nn.Parameter(torch.zeros(d_model))
        self.eps=eps

    def forward(self,x):
        mean=x.mean(-1,keepdim=True)
        var=x.var(-1,unbiased=False,keepdim=True)
        out=(x-mean)/torch.sqrt(var+self.eps)
        out=self.gamma*out+self.beta
        return out
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, hidden, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, hidden)
        self.fc2 = nn.Linear(hidden, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x
class Multi_head_attention(nn.Module):
    def __init__(self,d_model,n_head)->None:  #->è¡¨ç¤ºå‡½æ•°æ— è¿”å›å€¼  d_model:æ¨¡å‹çš„éšè—å±‚ç»´åº¦ n_head:æ³¨æ„åŠ›å¤´çš„æ•°é‡
        super(Multi_head_attention,self).__init__()
        # å°†ä¼ å…¥çš„å‚æ•°ä¿å­˜ä¸ºç±»å±æ€§ï¼Œåç»­åœ¨forwardä¸­ä½¿ç”¨
        self.n_head=n_head
        self.d_model=d_model

        #åˆ›å»ºä¸‰ä¸ªçº¿æ€§å˜æ¢å±‚ï¼Œå°†è¾“å…¥å‘é‡æŠ•å½±åˆ°QKVç©ºé—´
        self.w_q=nn.Linear(d_model,d_model)
        self.w_k=nn.Linear(d_model,d_model)
        self.w_v=nn.Linear(d_model,d_model)

        # æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥åä»ä¸ºd_model
        self.w_combine=nn.Linear(d_model,d_model) #çº¿æ€§æ˜ å°„å±‚
        self.softmax=nn.Softmax(dim=-1)

    def forward(self,q,k,v,mask=None):
        batch,time,dimension=q.shape
        n_d=self.d_model//self.n_head
        q,k,v=self.w_q(q),self.w_k(k),self.w_v(v)
        #æŠŠå‘é‡æ‹†åˆ†æˆå¤šä¸ªå¤´(batch,time,dimension)->(batch,time,n_head,n_d)
        q=q.view(batch,time,self.n_head,n_d).permute(0,2,1,3)
        k=k.view(batch,time,self.n_head,n_d).permute(0,2,1,3)
        v=v.view(batch,time,self.n_head,n_d).permute(0,2,1,3)
     
        score=q@k.transpose(2,3)/math.sqrt(n_d)
        if mask is not None:

            # mask=torch.tril(torch.ones(time,time,dtype=bool))
            score=score.masked_fill(mask==0,float("-inf"))

        score=self.softmax(score)@v

        score=score.permute(0,2,1,3).contiguous().view(batch,time,dimension)

        output=self.w_combine(score)
        return output
    
attention=Multi_head_attention(d_model,n_head)
output=attention(X,X,X)
print(output,output.shape)
class EncoderLayer(nn.Module):
    def __init__(self,d_model,ffn_hidden,n_head,drop_prob)->None:
        super(EncoderLayer,self).__init__()
        self.attention=Multi_head_attention(d_model,n_head)
        self.norm1=LayerNorm(d_model)
        self.drop1=nn.Dropout(drop_prob)

        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)
        self.norm2=LayerNorm(d_model)
        self.drop2=nn.Dropout(drop_prob)

    def forward(self,x,mask=None):
        _x=x
        x=self.attention(x,x,x,mask)
        
        x=self.drop1(x)
        x=self.norm1(x+_x)
        
        _x=x
        x=self.ffn(x)

        x=self.drop2(x)        
        x=self.norm2(x+_x)
        return x
class DecoderLayer(nn.Module):
    def __init__(self,d_model,ffn_hidden,n_head,drop_prob):
        super(DecoderLayer,self).__init__()
        self.attention=Multi_head_attention(d_model,n_head)
        self.norm1=LayerNorm(d_model)
        self.dropout1=nn.Dropout(drop_prob)

        self.cross_attention=Multi_head_attention(d_model,n_head)
        self.norm2=LayerNorm(d_model)
        self.dropout2=nn.Dropout(drop_prob)

        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)
        self.norm3=LayerNorm(d_model)
        self.dropout3=nn.Dropout(drop_prob)

    def forward(self,dec,enc,time_mask,s_mask):
        _x=dec
        x=self.attention(dec,dec,dec,time_mask) # ä¸‹ä¸‰è§’æ©ç ï¼ˆå› æœæ©ç ï¼‰

        x=self.dropout1(x)
        x=self.norm1(x+_x)

        if enc is not None:
            _x=x
            x=self.cross_attention(x,enc,enc,s_mask) #ä½ç½®æ©ç 

            x=self.dropout2(x)
            x=self.norm2(x+_x)

        #å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        _x=x
        x=self.ffn(x)
        x=self.dropout3(x)
        x=self.norm3(x+_x)
        return x
class Encoder(nn.Module):
    def __init__(self,env_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):
        super(Encoder, self).__init__()

        self.embedding = TransformerEmbedding(
            env_voc_size, d_model, max_len, drop_prob, device
        )

        self.layers = nn.ModuleList(
            [
                EncoderLayer(d_model, ffn_hidden, n_head, drop_prob)
                for _ in range(n_layer)
            ]
        )

    def forward(self, x, s_mask):
        x = self.embedding(x)
        for layer in self.layers:
            x = layer(x, s_mask)
        return x
class Decoder(nn.Module):
    def __init__(self, dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):
        super(Decoder,self).__init__()

        self.embedding=TransformerEmbedding(dec_voc_size, d_model, max_len, drop_prob, device)

        self.layers=nn.ModuleList(
            [DecoderLayer(d_model,ffn_hidden,n_head,drop_prob)for _ in range(n_layer)] 
        )
        self.fc=nn.Linear(d_model,dec_voc_size)

    def forward(self,dec,enc,time_mask,s_mask):
        dec=self.embedding(dec)
        for layer in self.layers:
            dec=layer(dec,enc,time_mask,s_mask)
        dec=self.fc(dec)
        return dec
class Transformer(nn.Module):
    def __init__(self, src_pad_idx,trg_pad_idx,enc_voc_size,dec_voc_size,max_len,d_model,n_head,ffn_hidden,n_layers,drop_prob,device):
        super(Transformer,self).__init__()

        self.encoder=Encoder(enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layers,drop_prob,device)
        self.decoder = Decoder(dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device)


        self.src_pad_idx=src_pad_idx
        self.trg_pad_idx=trg_pad_idx
        self.device=device

    def make_pad_mask(self,q,k,pad_idx_q,pad_idx_k):
        len_q,len_k=q.size(1),k.size(1)

        # ç»´åº¦(batch,time,len_q,len_k)
        q=q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)
        q=q.repeat(1,1,1,len_k)

        k=k.ne(pad_idx_k).unsqueeze(1).unsqueeze(3)
        k=k.repeat(1,1,len_q,1)

        mask=q&k  #å…¨ä¸€åˆ™ä¸€
        return mask


    def make_causal_mask(self,q,k):
        len_q,len_k=q.size(1),k.size(1)
        mask=torch.tril(torch.ones(len_q,len_k)).type(torch.BoolTensor).to(self.device)
        return mask
    
    def forward(self,src,trg):
        src_mask=self.make_pad_mask(src,src,self.src_pad_idx,self.src_pad_idx)
        trg_mask=self.make_pad_mask(trg,trg,self.trg_pad_idx,self.trg_pad_idx)*self.make_causal_mask(trg,trg)
        src_trg_mask=self.make_pad_mask(trg,src,self.trg_pad_idx,self.src_pad_idx)

        enc=self.encoder(src,src_mask)
        output=self.decoder(trg,enc,trg_mask,src_trg_mask)
        return output