{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94893479",
   "metadata": {},
   "source": [
    "•涌现能力：模型规模达到某一阈值后性能显著提升（非线性突变，而不是简单线性提升）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a874b",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109d021",
   "metadata": {},
   "source": [
    "## 为什么不使用整词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9bfb7",
   "metadata": {},
   "source": [
    "1.词表爆炸 2.无法处理新词，即OOV问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032c04f",
   "metadata": {},
   "source": [
    "## 分词方法：BPE、Wordpiece、Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29df84",
   "metadata": {},
   "source": [
    "BPE:1.将文本拆分成最小单元（字母、标点、单个汉字） 2.将频率最高的符号对合并再添加到词汇表，直至达到预定词表的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f83033",
   "metadata": {},
   "source": [
    "Wordpiece：每次找出相邻出现频率最高的一对子词，把它当作一个新子词加入词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b1568",
   "metadata": {},
   "source": [
    "<img src=\"wordpiece.png\" alt=\"图片描述\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf44523",
   "metadata": {},
   "source": [
    "Unigram：（在词表中删除词元）每次删除未被使用或出现概率低的子词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16155cf4",
   "metadata": {},
   "source": [
    "## BERT为何适合做理解任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028a055",
   "metadata": {},
   "source": [
    "• 掩码mask机制：在输入句子中随机掩盖token，让模型预测mask的实际token（模型必须读懂前后文才能预测词，强化了上下文理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289553c",
   "metadata": {},
   "source": [
    "•双向transformer结构使得模型可以从两个方向理解语意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3363881",
   "metadata": {},
   "source": [
    "<img src=\"GPT_and_BERT.png\" alt=\"图片描述\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728d384",
   "metadata": {},
   "source": [
    "•GPT为因果语言模型，其掩码机制采用下三角掩码，防止模型偷看未来的词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8e0ce",
   "metadata": {},
   "source": [
    "## RLHF是如何让模型输出符合人类偏好的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4e7bd",
   "metadata": {},
   "source": [
    "•RLHF流程：原始模型-->SFT-->RM-->PPO-->RLHF模型 (用人类反馈训练一个奖励函数，再用强化学习让模型优化输出，从而让它更符合人类偏好。)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064f752",
   "metadata": {},
   "source": [
    "## SFT与RLHF的优缺点对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0da41f",
   "metadata": {},
   "source": [
    "<img src=\"SFT_and_RLHF.png\" alt=\"图片描述\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4685fc",
   "metadata": {},
   "source": [
    "## 提示学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddf758",
   "metadata": {},
   "source": [
    "<img src=\"prompt learning.png\" alt=\"图片描述\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e758ccb",
   "metadata": {},
   "source": [
    "## 智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc3aa1",
   "metadata": {},
   "source": [
    "•智能体=LLM+记忆+规划+执行+反馈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddef3ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
